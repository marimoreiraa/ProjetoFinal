{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_PySpark",
      "provenance": [],
      "collapsed_sections": [
        "wox7CNOGJGrF",
        "QVW92KejMHoY",
        "Z-SnL3GEesv8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianaDuartee/ProjetoFinal/blob/main/2_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wox7CNOGJGrF"
      },
      "source": [
        "### INSTALANDO DEPENDECIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG4awT5LJEmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb24efd8-1312-448a-d8f4-fb402f5d0b63"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install gcsfs\n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/dist-packages (2021.11.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.35.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.18.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2021.11.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2021.11.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.7.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.10.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (2.0.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (4.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (21.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.7.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (1.0.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.26.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVW92KejMHoY"
      },
      "source": [
        "### IMPORTANDO BIBLIOTECAS, ABRINDO SPARKSESSION E CONFIGURANDO CHAVE DE SERVIÇO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA3IQkKCL2sI"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SQLContext\n",
        "from google.cloud import storage\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.types import DateType\n",
        "from pyspark.sql import Window\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "import os\n",
        "import gcsfs\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9olZPDMpxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3260782-84b6-4c35-ca7f-6fa4d3d02227"
      },
      "source": [
        "spark = SparkSession.builder\\\n",
        ".master('local')\\\n",
        ".appName('Projeto_Final')\\\n",
        ".config('spark.ui.enable', 'true')\\\n",
        ".config('spark.ui.port', '4050')\\\n",
        ".getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "serviceaccount = '/content/soulcode-projeto-final-4b88bea6e07a.json'\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = serviceaccount\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f5b6e17c649c:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Projeto_Final</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f621fdec510>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7EKwiUaOZ7N"
      },
      "source": [
        "### IMPORTANDO, EXTRAINDO INFORMAÇÕES E ANALISANDO DATAFRAMES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjEFDl68OhtS"
      },
      "source": [
        "'''https://acervolima.com/como-converter-pandas-para-pyspark-dataframe/'''\n",
        "\n",
        "# ARQUIVO 1\n",
        "file_path_1 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_despesas_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_1):\n",
        "    data = pd.read_csv(file_path_1, sep=',', encoding='UTF-8', header=0)\n",
        "\n",
        "df_1 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 2\n",
        "file_path_2 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_ocorrencias_normalizado.json'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_2):\n",
        "    data = pd.read_json(file_path_2, encoding='UTF-8')\n",
        "\n",
        "df_2 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 3\n",
        "file_path_3 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_vitimas_normalizado.json'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_3):\n",
        "    data = pd.read_json(file_path_3, encoding='UTF-8')\n",
        "\n",
        "df_3 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 5\n",
        "file_path_5 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_ocorrencia_vitimas_porAnoEstado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_5):\n",
        "    data = pd.read_csv(file_path_5, sep=',', encoding='UTF-8')\n",
        "\n",
        "df_5 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# ARQUIVO 6\n",
        "file_path_6 = 'gs://data_lake_ingest_data/1_input/Tabela_frequencia_escolar.xlsx'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_6):\n",
        "    data = pd.read_excel(file_path_6, header=0)\n",
        "\n",
        "df_6 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# ARQUIVO 7\n",
        "file_path_7 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_taxa_analfabetismo_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_7):\n",
        "    data = pd.read_csv(file_path_7, header=0)\n",
        "\n",
        "df_7 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "df_1.printSchema()\n",
        "df_1.show(5, truncate=False)\n",
        "df_1.dtypes\n",
        "\n",
        "df_2.printSchema()\n",
        "df_2.show(5, truncate=False)\n",
        "df_2.dtypes\n",
        "\n",
        "df_3.printSchema()\n",
        "df_3.show(5, truncate=False)\n",
        "df_3.dtypes\n",
        "\n",
        "df_4.printSchema()\n",
        "df_4.show(5, truncate=False)\n",
        "df_4.dtypes\n",
        "\n",
        "df_5.printSchema()\n",
        "df_5.show(5, truncate=False)\n",
        "df_5.dtypes\n",
        "\n",
        "df_6.printSchema()\n",
        "df_6.show(5, truncate=False)\n",
        "df_6.dtypes\n",
        "\n",
        "df_7.printSchema()\n",
        "df_7.show(5, truncate=False)\n",
        "df_7.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsZ927gDOKDx"
      },
      "source": [
        "### NORMALIZANDO DATAFRAMES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLjq07hvfqKL"
      },
      "source": [
        "DATAFRAME 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAPe60SHVbP6"
      },
      "source": [
        "# DATAFRAME 1\n",
        "\n",
        "df_1 = df_1.drop('Estimativa_2021', 'Previsao2021Media')\n",
        "df_1 = df_1.drop('Estimativa_2021', 'Previsao2021Media')\n",
        "df_1 = df_1.withColumnRenamed('Media Anual', 'Estimativa_Despesa_2021')\n",
        "df_1 = df_1.withColumnRenamed('Previsao2021|Media', 'Previsao2021_Media')\n",
        "\n",
        "df_1.withColumn(\"Despesas2017\", F.round(df_1.Despesas2016.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2017\", F.round(df_1.Despesas2017.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2018\", F.round(df_1.Despesas2018.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2019\", F.round(df_1.Despesas2019.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2020\", F.round(df_1.Despesas2020.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Previsao2021_Media\", F.round(df_1.Previsao2021_Media.cast(FloatType()), 3))\n",
        "\n",
        "df_1.show()\n",
        "df_1.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oILuSrSftla"
      },
      "source": [
        "DATAFRAME 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySfZMio_fkY9"
      },
      "source": [
        "'''https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/'''\n",
        "\n",
        "df_3 = df_3.toPandas()\n",
        "df_3.to_csv('temp_pyspark_tabela_vitimas_sexo.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_tabela_vitimas_sexo.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_tabela_vitimas_sexo.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V-KKecgfw6R"
      },
      "source": [
        "DATAFRAME 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfQ55azXerU3"
      },
      "source": [
        "# DROPANDO COLUNAS\n",
        "df_6 = df_6.drop('15 a 17 anos - 2019', \n",
        "                 '15 a 17 anos - 2018',\n",
        "                 '15 a 17 anos - 2017',\n",
        "                 '15 a 17 anos - 2016',\n",
        "                 '18 a 24 anos - 2019',\n",
        "                 '18 a 24 anos - 2018',\n",
        "                 '18 a 24 anos - 2017',\n",
        "                 '18 a 24 anos - 2016',\n",
        "                 '25 anos ou mais - 2019',\n",
        "                 '25 anos ou mais - 2018',\n",
        "                 '25 anos ou mais - 2017',\n",
        "                 '25 anos ou mais - 2016')\n",
        "\n",
        "# RENOMEANDO COLUNAS FAIXA DE IDADE\n",
        "df_6 = df_6.withColumnRenamed('Grandes Regiões, Unidades da Federação e Municípios das Capitais', 'UF')\n",
        "df_6 = df_6.withColumnRenamed('Media 15 a 17 anos', 'Media_Freq_15_a_17_anos')\n",
        "df_6 = df_6.withColumnRenamed('média 18 a 24 anos', 'Media_Freq_18_a_24_anos')\n",
        "df_6 = df_6.withColumnRenamed('média 25 anos ou mais', 'Media_Freq_25_anos_ou_mais')\n",
        "\n",
        "# FILTRO PARA REMOVER REGIÕES E CAPITAIS\n",
        "df_6 = df_6.filter(\n",
        "            (df_6.UF != \"Brasil\") & \n",
        "            (df_6.UF != \"Norte\") & \n",
        "            (df_6.UF != \"Nordeste\") &\n",
        "            (df_6.UF != \"Sudeste\") & \n",
        "            (df_6.UF != \"Sul\") &\n",
        "            (df_6.UF != \"Centro-Oeste\") &\n",
        "            (df_6.UF != \"Porto Velho\") &\n",
        "            (df_6.UF != \"Rio Branco\") &\n",
        "            (df_6.UF != \"Manaus\") &\n",
        "            (df_6.UF != \"Boa Vista\") &\n",
        "            (df_6.UF != \"Macapá\") &\n",
        "            (df_6.UF != \"Palmas\") &\n",
        "            (df_6.UF != \"São Luiz\") &\n",
        "            (df_6.UF != \"Teresina\") &\n",
        "            (df_6.UF != \"Fortaleza\") &\n",
        "            (df_6.UF != \"Natal\") &\n",
        "            (df_6.UF != \"joão Pessoa\") &\n",
        "            (df_6.UF != \"Recife\") &\n",
        "            (df_6.UF != \"Maceió\") &\n",
        "            (df_6.UF != \"Aracaju\") &\n",
        "            (df_6.UF != \"Salvador\") &\n",
        "            (df_6.UF != \"Belo Horizonte\") &\n",
        "            (df_6.UF != \"Vitória\") &\n",
        "            (df_6.UF != \"Curitiba\") &\n",
        "            (df_6.UF != \"Florianópolis\") &\n",
        "            (df_6.UF != \"Porto Alegre\") &\n",
        "            (df_6.UF != \"Campo Grande\") &\n",
        "            (df_6.UF != \"Cuiabá\") &\n",
        "            (df_6.UF != \"Goiânia\") &\n",
        "            (df_6.UF != \"Brasilia\")\n",
        "            )\\\n",
        "            .orderBy('UF', ascending=True)\n",
        "\n",
        "# # INCREMENTANDO TAXA DE EVASÃO\n",
        "# df_6 = df_6.withColumn('Evasao_11_a_14_anos', (df_5.Freq_11_a_14_anos - 100)) \n",
        "# df_6 = df_6.withColumn('Evasao_15_a_17_anos', (df_5.Freq_11_a_14_anos - 100))\n",
        "# df_6 = df_6.withColumn('Evasao_18_a_24_anos', (df_5.Freq_11_a_14_anos - 100))\n",
        "# df_6 = df_6.withColumn('Evasao_25_anos_ou_mais', (df_5.Freq_11_a_14_anos - 100))\n",
        "\n",
        "# CONVERTENDO VALORES EM PORCENTAGEM\n",
        "'''https://stackoverflow.com/questions/60673912/how-to-convert-number-into-percentage'''\n",
        "\n",
        "df_6 = df_6\\\n",
        ".withColumn(\"Media_Freq_15_a_17_anos\", F.concat((F.col(\"Media_Freq_15_a_17_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"Media_Freq_18_a_24_anos\", F.concat((F.col(\"Media_Freq_18_a_24_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"Media_Freq_25_anos_ou_mais\", F.concat((F.col(\"Media_Freq_25_anos_ou_mais\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "# .withColumn(\"Media_Freq_Escolar\", F.concat((F.col(\"Media_Freq_Escolar\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_11_a_14_anos\", F.concat((F.col(\"Evasao_11_a_14_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_15_a_17_anos\", F.concat((F.col(\"Evasao_15_a_17_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_18_a_24_anos\", F.concat((F.col(\"Evasao_18_a_24_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_25_anos_ou_mais\", F.concat((F.col(\"Evasao_25_anos_ou_mais\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "\n",
        "# AJUSTANDO EXIBIÇÃO DO DATAFRAME\n",
        "df_6 = df_6.select('UF', \n",
        "                   'Media_Freq_15_a_17_anos', \n",
        "                #    'Evasao_11_a_14_anos',\n",
        "                   'Media_Freq_18_a_24_anos',\n",
        "                #    'Evasao_15_a_17_anos',\n",
        "                   'Media_Freq_25_anos_ou_mais',\n",
        "                #    'Evasao_18_a_24_anos',\n",
        "                #    'Freq_25_anos_ou_mais',\n",
        "                #    'Evasao_25_anos_ou_mais',\n",
        "                #    'Media_Freq_Escolar'\n",
        "               )\n",
        "\n",
        "df_6.show(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU5Gz3ShKsoA"
      },
      "source": [
        "# TRANSFORMARDO DATAFRAME PYSPAK EM PANDAS\n",
        "'''https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/'''\n",
        "\n",
        "df_6 = df_6.toPandas()\n",
        "df_6.to_csv('temp_pyspark_tabela_frequencia_escolar_normalizado.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_tabela_frequencia_escolar_normalizado.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_tabela_frequencia_escolar_normalizado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEWvGtHlf3eZ"
      },
      "source": [
        "DATAFRAME 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Flsf5qzGHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e16048d-b9a3-45a4-eeee-b0a84dd20217"
      },
      "source": [
        "# DATAFRAME 7\n",
        "df_7 = df_7.drop('Grupo_idade')\n",
        "df_7 = df_7.drop('Unnamed: 0')\n",
        "\n",
        "df_7 = df_7\\\n",
        ".withColumn(\"TaxaAnalfabetismo2016\", F.concat((F.col(\"TaxaAnalfabetismo2016\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2017\", F.concat((F.col(\"TaxaAnalfabetismo2017\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2018\", F.concat((F.col(\"TaxaAnalfabetismo2018\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2019\", F.concat((F.col(\"TaxaAnalfabetismo2019\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "\n",
        "df_7.show()\n",
        "\n",
        "# TRANSFORMARDO DATAFRAME PYSPAK EM PANDAS\n",
        "'''https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/'''\n",
        "\n",
        "df_6 = df_6.toPandas()\n",
        "df_6.to_csv('temp_pyspark_tabela_frequencia_escolar_normalizado.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_tabela_frequencia_escolar_normalizado.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_tabela_frequencia_escolar_normalizado.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|                 UF|TaxaAnalfabetismo2016|TaxaAnalfabetismo2017|TaxaAnalfabetismo2018|TaxaAnalfabetismo2019|\n",
            "+-------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|           Rondônia|                  6 %|                  7 %|                  6 %|                  6 %|\n",
            "|               Acre|                 13 %|                 12 %|                 12 %|                 11 %|\n",
            "|           Amazonas|                  6 %|                  6 %|                  5 %|                  5 %|\n",
            "|            Roraima|                  6 %|                  6 %|                  6 %|                  5 %|\n",
            "|               Pará|                  9 %|                  8 %|                  8 %|                  8 %|\n",
            "|              Amapá|                  5 %|                  5 %|                  6 %|                  5 %|\n",
            "|          Tocantins|                 10 %|                 10 %|                 10 %|                  9 %|\n",
            "|           Maranhão|                 16 %|                 16 %|                 16 %|                 15 %|\n",
            "|              Piauí|                 17 %|                 16 %|                 16 %|                 16 %|\n",
            "|              Ceará|                 15 %|                 14 %|                 13 %|                 13 %|\n",
            "|Rio Grande do Norte|                 14 %|                 13 %|                 12 %|                 13 %|\n",
            "|            Paraíba|                 16 %|                 16 %|                 16 %|                 16 %|\n",
            "|         Pernambuco|                 12 %|                 13 %|                 11 %|                 11 %|\n",
            "|            Alagoas|                 19 %|                 18 %|                 17 %|                 17 %|\n",
            "|            Sergipe|                 14 %|                 14 %|                 13 %|                 13 %|\n",
            "|              Bahia|                 13 %|                 12 %|                 12 %|                 12 %|\n",
            "|       Minas Gerais|                  6 %|                  6 %|                  5 %|                  5 %|\n",
            "|     Espírito Santo|                  6 %|                  5 %|                  5 %|                  5 %|\n",
            "|     Rio de Janeiro|                  2 %|                  2 %|                  2 %|                  2 %|\n",
            "|          São Paulo|                  2 %|                  2 %|                  2 %|                  2 %|\n",
            "+-------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-SnL3GEesv8"
      },
      "source": [
        "### CONSULTAS / AGREGANDO DADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2bgQFezOJDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9827d51a-e3d8-4f22-8fab-6164f62c74f1"
      },
      "source": [
        "# VISUALIZAR DATAFRAMES TRATADOS\n",
        "\n",
        "df_1.show(5, truncate=False)\n",
        "df_2.show(5, truncate=False)\n",
        "df_3.show(5, truncate=False)\n",
        "df_4.show(5, truncate=False)\n",
        "df_5.show(5, truncate=False)\n",
        "df_6.show(5, truncate=False)\n",
        "df_7.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+------------+------------------+------------+------------------+---------+------------------+\n",
            "|UF      |Despesas2016      |Despesas2017|Despesas2018      |Despesas2019|Despesas2020      |Variacao%|Previsao2021_Media|\n",
            "+--------+------------------+------------+------------------+------------+------------------+---------+------------------+\n",
            "|Acre    |498.535           |568.361     |627.351           |692.57      |493.75300000000004|8.5      |576.114           |\n",
            "|Amapá   |475.32300000000004|500.227     |489.228           |606.366     |752.252           |52.3     |564.6792          |\n",
            "|Amazonas|1604.7320000000002|1578.957    |1862.6689999999999|2108.46     |2101.447          |60.4     |1851.253          |\n",
            "|Pará    |2551.741          |2579.916    |2903.2            |2943.448    |2966.553          |58.0     |2788.9716         |\n",
            "|Rondônia|874.8739999999999 |895.868     |903.2539999999999 |853.971     |999.044           |-14.7    |905.4022          |\n",
            "+--------+------------------+------------+------------------+------------+------------------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "|UF  |TipoCrime                      |Ano |Mes    |Ocorrencias|\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "|Acre|Estupro                        |2021|janeiro|39         |\n",
            "|Acre|Furto de veículo               |2021|janeiro|55         |\n",
            "|Acre|Homicídio doloso               |2021|janeiro|14         |\n",
            "|Acre|Lesão corporal seguida de morte|2021|janeiro|0          |\n",
            "|Acre|Roubo a instituição financeira |2021|janeiro|0          |\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----+----------------+----+---------+----------+-------+\n",
            "|UF  |TipoCrime       |Ano |Mes      |SexoVitima|Vitimas|\n",
            "+----+----------------+----+---------+----------+-------+\n",
            "|Acre|Homicídio doloso|2021|janeiro  |Feminino  |1      |\n",
            "|Acre|Homicídio doloso|2021|janeiro  |Masculino |13     |\n",
            "|Acre|Homicídio doloso|2021|janeiro  |Sexo NI   |0      |\n",
            "|Acre|Homicídio doloso|2021|fevereiro|Feminino  |4      |\n",
            "|Acre|Homicídio doloso|2021|fevereiro|Masculino |12     |\n",
            "+----+----------------+----+---------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "|UF  |TipoCrime                      |Ano |Mes    |Ocorrencias|\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "|Acre|Estupro                        |2021|janeiro|39         |\n",
            "|Acre|Furto de veículo               |2021|janeiro|55         |\n",
            "|Acre|Homicídio doloso               |2021|janeiro|14         |\n",
            "|Acre|Lesão corporal seguida de morte|2021|janeiro|0          |\n",
            "|Acre|Roubo a instituição financeira |2021|janeiro|0          |\n",
            "+----+-------------------------------+----+-------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----+----+-----------------------------------+-----------+-------+\n",
            "|UF  |Ano |TipoCrime                          |Ocorrencias|Vitimas|\n",
            "+----+----+-----------------------------------+-----------+-------+\n",
            "|Acre|2016|Homicídio doloso                   |425        |354    |\n",
            "|Acre|2017|Homicídio doloso                   |221        |232    |\n",
            "|Acre|2017|Lesão corporal seguida de morte    |0          |3      |\n",
            "|Acre|2017|Roubo seguido de morte (latrocínio)|10         |15     |\n",
            "|Acre|2018|Homicídio doloso                   |396        |394    |\n",
            "+----+----+-----------------------------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eb88fa27e732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHFqf-0VAulA"
      },
      "source": [
        "JOINS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsswAo5erQl-"
      },
      "source": [
        "# AGREGAÇÃO 1\n",
        "\n",
        "#AGRUPANDO VALORES DOS DATAFRAMES\n",
        "df_ins2 = df_2.groupBy('UF').sum('Ocorrencias').orderBy('UF')\n",
        "df_ins2.show(50, truncate=False)\n",
        "\n",
        "df_ins3 = df_3.groupBy('UF').sum('Vitimas').orderBy('UF')\n",
        "df_ins3.show(50, truncate=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 1\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS E VITIMAS\n",
        "\n",
        "df_join_1 = df_ins2.join(df_ins3, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)')\\\n",
        ".orderBy(F.asc('UF'))\n",
        "\n",
        "df_join_1.show(50, False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 2\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS, VITIMAS, MEDIA DE FREQ ESCOLAR\n",
        "\n",
        "df_join_1 = df_join_1.join(df_6, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy('UF', ascending=True)\n",
        "\n",
        "df_join_1.show(50, False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 3\n",
        "# AGREGA OS VALORES TOTAIS DE DESPESAS GOVERNAMENTAIS, OCORRENCIAS, VITIMAS, MEDIA DE FREQ. ESCOLAR\n",
        "\n",
        "df_join_1 = df_join_1.join(df_1, on=['UF'], how='inner')\\\n",
        ".select('UF', 'Despesas2020', 'sum(Ocorrencias)', 'sum(Vitimas)', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy(F.asc('UF'))\n",
        "\n",
        "df_join_1\\\n",
        ".orderBy(\"Despesas2020\", ascending=False)\\\n",
        ".show(50, False)\n",
        "\n",
        "df_join_1.dtypes\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_1_General_Data.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_1_General_Data.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_1_General_Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TofPZBof0Npu"
      },
      "source": [
        "# AGREGAÇÃO 2\n",
        "\n",
        "# JOIN 1\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS E VITIMAS POR ESTADO \n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_6.join(df_7, on=['UF'], how='inner')\\\n",
        ".select('UF', 'Media_Freq_15_a_17_anos', 'Media_Freq_18_a_24_anos', 'Media_Freq_25_anos_ou_mais', \n",
        "        'TaxaAnalfabetismo2016', 'TaxaAnalfabetismo2017', 'TaxaAnalfabetismo2018', 'TaxaAnalfabetismo2019')\\\n",
        ".orderBy(F.asc('UF'))\\\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWfPjmVJkZTR"
      },
      "source": [
        "# AGREGAÇÃO 3\n",
        "\n",
        "# AGRUPANDO VALORES DOS DATAFRAMES\n",
        "df_agg_1 = df_2.filter(df_2.Ano == 2020)\\\n",
        ".groupBy('UF').sum('Ocorrencias').alias('Ocorrencias_2020').orderBy('UF')\n",
        "\n",
        "df_agg_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_agg_2 = df_3.filter(df_3.Ano == 2020)\\\n",
        ".groupBy('UF').sum('Vitimas').alias('Vitimas_2020').orderBy('UF')\n",
        "\n",
        "df_agg_2.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# AGREGANDO OS VALORES PELO ANO DE 2020\n",
        "\n",
        "df_join_1 = df_agg_1.join(df_agg_2, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)')\\\n",
        ".orderBy('UF', ascending=True)\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_2 = df_1.select('UF', 'Despesas2020')\n",
        "\n",
        "df_join_1 = df_join_1.join(df_join_2, on=['UF'], how='inner')\\\n",
        ".select('UF', \n",
        "        'sum(Ocorrencias)', \n",
        "        'sum(Vitimas)', \n",
        "        'Despesas2020')\n",
        "\n",
        "df_join_1 = df_join_1\\\n",
        ".withColumnRenamed('sum(Ocorrencias)', 'Ocorrencias_2020')\\\n",
        ".withColumnRenamed('sum(Vitimas)', 'Vitimas_2020')\\\n",
        ".withColumnRenamed('Despesas2020', 'Despesas_Gov_2020')\\\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_3_general_data_2020.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_3_general_data_2020.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_3_general_data_2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8894hv-Gpfam"
      },
      "source": [
        "WINDOW FUNCTIONS RANKING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpk-tMEv6sP7"
      },
      "source": [
        "# ANALISANDO AS OCORRENCIAS DO ESTADO SP EM 2019\n",
        "\n",
        "df_2_consulta = df_2.groupby('UF', 'TipoCrime', 'Ano')\\\n",
        ".sum('Ocorrencias')\\\n",
        ".filter(\n",
        "    (df_2.UF == \"São Paulo\") & \n",
        "    (df_2.Ano == 2019))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# RANKING DAS OCORRENCIAS\n",
        "w0 = Window.partitionBy(F.col('UF')).orderBy(F.desc('sum(Ocorrencias)'))\n",
        "\n",
        "df_2_consulta.withColumn('Ranking', F.row_number().over(w0))\\\n",
        ".show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYSQUkOqpkkm"
      },
      "source": [
        "STRUCT TYPE | UNION BY NAME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ySur1R_FzeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "abf76391-8a29-4145-91e7-b657845c8ddf"
      },
      "source": [
        "# CRIANDO DATAFRAME VAZIO\n",
        "\n",
        "esquema = (StructType([\n",
        "        StructField('UF', StringType(), True),\n",
        "        StructField('TipoCrime', StringType(), True),\n",
        "        StructField('Ano', IntegerType(), True),\n",
        "        StructField('Mes', StringType(), True),\n",
        "        StructField('SexoVitima', StringType(), True),\n",
        "        StructField('Vitimas', IntegerType(), True)\n",
        "    ])\n",
        ")\n",
        "\n",
        "df_3_consulta = spark.createDataFrame(data='', schema=esquema)\n",
        "df_3_consulta.printSchema()\n",
        "df_3_consulta.show()\n",
        "\n",
        "# UNINDO DF\n",
        "\n",
        "df_3_consulta.unionByName(df_3)\\\n",
        ".orderBy('Ano', ascending=False)\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# EXIBINDO AS QTD. DE VITIMAS DE CRIMES DO ESTADO RJ EM 2020 POR SEXO\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Feminino\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Masculino\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Sexo NI\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7e01d8e876a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CRIANDO DATAFRAME VAZIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m esquema = (StructType([\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TipoCrime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZQVT_sss3Rf"
      },
      "source": [
        "HEADER | AGGREGATE | SUM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyui-5cpTgOT"
      },
      "source": [
        "# ALTERANDO CABEÇALHO\n",
        "\n",
        "file_path_1 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_despesas_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_1):\n",
        "    data = pd.read_csv(file_path_1,\n",
        "                       sep=',', \n",
        "                       encoding='UTF-8')\n",
        "\n",
        "esquema=['UF', \n",
        "         'Desp_2016', \n",
        "         'Desp_2017', \n",
        "         'Desp_2018', \n",
        "         'Desp_2019', \n",
        "         'Desp_2020', \n",
        "         'Variacao_%', \n",
        "         'Previsao_2021_Media']\n",
        "\n",
        "df_1_consulta = spark.createDataFrame(data, schema=esquema)\n",
        "\n",
        "df_1_consulta.show(5, truncate=False)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# AGREGANDO O TOTAL DE DESPESAS GOVERNAMENTAIS DO PARANA REFERENTES AOS ULTIMOS 3 ANOS\n",
        "\n",
        "df_1_consulta.select('UF', 'Desp_2018', 'Desp_2019', 'Desp_2020')\\\n",
        ".filter((df_1_consulta.UF == 'Paraná'))\\\n",
        ".groupBy('UF', 'Desp_2018', 'Desp_2019', 'Desp_2020')\\\n",
        ".agg(F.sum(df_1_consulta.Desp_2018 + \n",
        "           df_1_consulta.Desp_2019 + \n",
        "           df_1_consulta.Desp_2020).alias('PR_Despesas_Totais_Milhoes'))\\\n",
        ".show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeID-8LSEBLn"
      },
      "source": [
        "FILTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdnQjVNVDTjd"
      },
      "source": [
        "# FILTRANDO INTERVALOS\n",
        "\n",
        "# IGUAL A 90% E MAIOR QUE 20 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_15_a_17_anos == '90 %') & \n",
        "    (df_6.Media_Freq_15_a_17_anos > '20 %'))\\\n",
        ".select('UF', 'Media_Freq_15_a_17_anos')\\\n",
        ".orderBy(F.desc('Media_Freq_15_a_17_anos'))\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# IGUAL E MENOR A 40 % OU IGUAL E MAIOR QUE 20 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_18_a_24_anos <= '40 %') |\n",
        "    (df_6.Media_Freq_18_a_24_anos >= '20 %'))\\\n",
        ".select('UF', 'Media_Freq_18_a_24_anos')\\\n",
        ".orderBy(F.desc('Media_Freq_18_a_24_anos'))\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# IGUAL E MENOR QUE 20 % OU MAIOR QUE 8 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_25_anos_ou_mais <= '20 %') |\n",
        "    (df_6.Media_Freq_25_anos_ou_mais > '8 %'))\\\n",
        ".select('UF', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy(F.desc('Media_Freq_25_anos_ou_mais'))\\\n",
        ".show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}